* Overview
** SQL
   Spark SQL 的一个用途是执行 SQL queries。Spark SQL 也可以用于从一个现存的 Hive 设备中读取数据。
** Datasets and DataFrames
   Dataset 是一个分布式数据集。Dataset 提供了 RDDs 的优点以及 Spark SQL 的优化执行引擎的优势。Dataset 可以通过 JVM 对象来构造并且熟练使用实用的变换 (map, flatmap, filter, etc)。Dataset API 可以使用 Scala 和 JAVA。 Python不支持 Dataset API。但是，由于 Python 的动态特点，许多 Dataset API 的优点已经可以使用了。R 也是类似情况。
   
   Data Frame 是把一个 Dataset 组织到命名的列中。它在概念上等价于关系数据库中的一个表。DataFrame 可以通过结构化数据文件，Hive 中的表，外部数据库或者现存的 RDDs。DataFrame API 支持 Scala，JAVA，Python 和 R。
* Getting Started
** Starting Point: SpeakSession
   Spark 中所有功能的入口点是 SparkSession 类。要创建一个基本的 SparkSession，只需使用 SparkSession.builder
   #+begin_src ipython
     from pyspark.sql import SparkSession

     spark = SparkSession.builder.appName("Python Spark SQL basic example").config(
         "spark.some.config.option", "some-value").getOrCreate()

   #+end_src
   SparkSession 在 Spark 2.0 中，为 Hive 特性包括可以使用 HiveQL 写 queries，访问 Hive UDFs，和 从 Hive tables 中读取数据的能力提供内置的支持。
** Creating DataFrames
   通过 SparkSession，应用可以从一个现存的 RDD，Hive table，Spark data sources 创建一个 DataFrames。
   #+begin_src ipython
     from pyspark.sql import SparkSession

     SPARK_HOME = "/Users/hengli1/Documents/dev/spark-2.2.0-bin-hadoop2.7"

     spark = SparkSession.builder.appName("Python Spark SQL basic example").config(
         "spark.some.config.option", "some-value").getOrCreate()

     df = spark.read.json(SPARK_HOME + "/examples/src/main/resources/people.json")
     df.show()

   #+end_src

** Untyped Dataset Operations
   在 Python 中可以访问一个 DataFrame 的列通过属性（df.age）或 index（df['age'])。推荐使用 index 的方式。

